#job名字，每个任务唯一,如果重复提交同名任务会提示任务已存在
jobName=local_test

#是否打开增量模式，如果设为true，会每间隔incrementalModeInterval(单位秒)重新扫描一次增量数据，并将增量数据同步到oss上,incrementalModeInterval不建议配置成小于3600秒的值，会浪费很多请求次数，造成额外的开销(目前可配置的最小间隔为900秒)
isIncremental=false
incrementalModeInterval=86400

##############################################################同步源端设置##############################################################
#同步源类型，目前支持：
#1.local(本地文件，此选项只需要填写srcPrefix，不需要填写srcAccessKey，srcSecretKey，srcDomain，srcBucket)
#2.oss(从oss的一个bucket迁移到另一个bucket)
#3.qiniu(七牛)
#4.bos(百度的云存储)
#5.ks3(金山的云存储)
#6.s3(亚马逊s3) 
#7.youpai(又拍云，又拍云获取文件列表的接口比较特殊，在同步时list不支持断点，因此在list没有完成时杀掉同步进程会导致下次会重新全部list文件列表)
#8.http (通过提供的http链接列表同步数据，此选项不需要填写srcAccessKey，srcSecretKey，srcDomain，srcBucket，srcPrefix）
#9.cos(腾讯云)
srcType=local

#源access key,同步本地文件以及通过http地址同步的不需要填,大部分云平台填写控制台获取的acceess key/accss key id,又拍云填写操作员账号
srcAccessKey=

#源secrect key,同步本地文件以及通过http地址同步的不需要填,大部分云平台填写控制台获取的secret key/access key secret,又拍云填写操作员密码
srcSecretKey=

#源endpoint,同步本地文件以及通过http地址同步的不需要填
#1.oss: 从控制台获取域名(非带bucket前缀的二级域名),域名列表参考https://help.aliyun.com/document_detail/31834.html; 例:"srcDomain=http://oss-cn-hangzhou-internal.aliyuncs.com"; 如果用阿里云ecs虚拟机做迁移的，请使用internal域名，不计费且不受虚拟机带宽限制(非虚拟机无法使用)；例：http://oss-cn-hangzhou-internal.aliyuncs.com 
#2.七牛: 从七牛控制台获取对应bucket的域名
#3.百度bos: http://bj.bcebos.com或者http://gz.bcebos.com
#4.金山ks3: http://kss.ksyun.com或者http://ks3-cn-beijing.ksyun.com或者http://ks3-us-west-1.ksyun.com 
#5.亚马逊s3: 各个region的地址请参考http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region 
#6.又拍云:  http://v0.api.upyun.com(自动判断最优线路)或者http://v1.api.upyun.com(电信线路)或者http://v2.api.upyun.com(联通网通线路)或者http://v3.api.upyun.com(移动铁通线路) 
#7.腾讯云: cos v3版本不需要填写;cos v4版本需要填上bucket所在的区域，比如华南园区填写:gz,华北园区:tj,华东园区:sh
srcDomain=

#源bucket名字，同步本地文件以及通过http地址同步的不需要填，不需要加上"/"
srcBucket=

#源前缀，默认为空，如果srcType=local，则此目录是本地目录，如果是其他类型，则是源Bucket里需要同步的Object的前缀,注意如果是本地目录，需要完整目录路径(以'/'进行分割并且以'/'结尾,例: /home/admin/example/)
srcPrefix=

##############################################################同步目的端设置################################################################
#目的 access key
destAccessKey=

#目的 secret key
destSecretKey=

#目的endpoint,请根据您的实际oss区域填写，默认为杭州的域名,如果用阿里云ecs虚拟机做迁移的，请使用internal域名，不计费且不受虚拟机带宽限制(非虚拟机无法使用)；例：http://oss-cn-hangzhou-internal.aliyuncs.com
#注意:域名里不要带上bucket前缀,oss域名帮助页面:https://help.aliyun.com/document_detail/31837.html
destDomain=http://oss-cn-hangzhou.aliyuncs.com

#目的bucket，不需要加上"/"
destBucket=

#目标前缀，默认为空，直接放在bucket下(如果要将数据同步到oss的某个目录下，请以'/'结尾）,注意:oss不支持以 '/' 作为文件的开头，所以destPrefix请不要配置以'/'做为开头
#一个本地文件路径为 srcPrefix + relativePath的文件，迁移到oss的路径为destDomain/destBucket/destPrefix + relativePath
#一个云端文件路径为 srcDomain/srcBucket/srcPrefix + relativePath的文件，迁移到oss的路径为destDomain/destBucket/destPrefix + relativePath
destPrefix=

##############################################################srcType=http时的配置选项########################################################
#srcType="http"时，需要提供http列表文件的绝对路径，此文件中的http链接需要划分成两列，分别代表前缀以及上传到oss后的相对路径
#例，有一个完整的http链接是127.0.0.1/aa/bb.jpg,不同的切分方法最后会导致上传到oss的路径会不一样： 
#c:/example/http.list文件内容：
#127.0.0.1/aa/              bb.jpg 
#127.0.0.1/                   aa/bb.jpg
# 第一行的文件导入到oss后的路径为 "destDomain/destBucket/destPrefix" + "bb.jpg"
# 第二行的文件导入到oss后的路径为 "destDomain/destBucket/destPrefix" + "aa/bb.jpg"

httpListFilePath=c:/example/http.list
httpPrefixColumn=1
relativePathColumn=2

##############################################################srcType=cos时的配置项###########################################################
#腾讯云的appId
appId=0

##############################################################任务配置，没有特殊需求不需要修改#################################################
#job类型(import/audit)，import为同步数据到oss，audit为校验源端数据和oss数据是否一致
jobType=import

#只导入源文件最后修改时间大于该时间的数据，默认为0，这个时间为unix时间戳（秒数）
importSince=0

#在校验时，如果文件的最后修改时间大于该值，则跳过此文件的校验，默认值0为关闭该功能，所有文件都需要校验，这个时间为unix时间戳（秒数），jobType为audit时此项不生效
lastModify=0

# 每个子任务最大的文件个数限制，这个会影响到任务执行的并行度，一般配置为总的文件数/120
taskObjectCountLimit=10000

#每个子任务下载的最大文件大小限制(bytes)
taskObjectSizeLimit=1000000000

#并行扫描文件列表的线程数，只影响扫描文件的效率,没有特殊需求不要修改
scanThreadCount=1

#最大允许并行扫描目录的深度，默认为1就是只能在顶级目录间并行扫描,没有特殊需求不要修改,随意配置的过大会导致任务无法正常运行
maxMultiThreadScanDepth=1
